{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_digits()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[100]\n",
    "dataset.images[100]\n",
    "dataset.target[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e69e0b2230>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYUklEQVR4nO3df3CUhZ3H8c+SNQtqsgISSMryQ0ERMCkS4NJo/QHC5ZDRzg1lGJxGqJ2TWQqYccbJ/VG86ZSlM9cO2mEiUAzOWAq206B1hBSohOlJJITLFPQGQamsIqT2ZPPj2gWzz/1x57YpEPJs8s3Ds7xfM89Md+fZPJ9hqG92N8kGHMdxBABAPxvk9QAAQHYiMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwERwoC+YSqV05swZ5eXlKRAIDPTlAQB94DiO2tvbVVRUpEGDen6OMuCBOXPmjCKRyEBfFgDQj+LxuEaPHt3jOQMemLy8PEnSvfonBXXDQF8ePpNz10SvJ2TkH176vdcTMvIfZUO8noBr3Be6qN/pzfR/y3sy4IH58mWxoG5QMEBg0LOcnJDXEzIy+GZ//t3m/5O4qv//7ZW9eYuDN/kBACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADCRUWA2bNigcePGafDgwZo1a5YOHTrU37sAAD7nOjA7duxQVVWV1qxZoyNHjqikpETz5s1Ta2urxT4AgE+5DsyPf/xjfec739HSpUs1efJkvfjii7rxxhv10ksvWewDAPiUq8BcuHBBzc3NmjNnzl+/wKBBmjNnjg4ePHjZxySTSbW1tXU7AADZz1VgPvvsM3V1dWnkyJHd7h85cqTOnj172cfEYjGFw+H0EYlEMl8LAPAN8+8iq66uViKRSB/xeNz6kgCAa0DQzcm33nqrcnJydO7cuW73nzt3TqNGjbrsY0KhkEKhUOYLAQC+5OoZTG5urqZPn659+/al70ulUtq3b5/Kysr6fRwAwL9cPYORpKqqKlVWVqq0tFQzZ87U+vXr1dnZqaVLl1rsAwD4lOvALFq0SH/84x/1ve99T2fPntVXv/pV7d69+5I3/gEA1zfXgZGkFStWaMWKFf29BQCQRfhdZAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMBERp8HAwyU408O9XpCRtaHj3g9ISMNKvd6ArIIz2AAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmHAdmAMHDmjBggUqKipSIBDQzp07DWYBAPzOdWA6OztVUlKiDRs2WOwBAGSJoNsHVFRUqKKiwmILACCLuA6MW8lkUslkMn27ra3N+pIAgGuA+Zv8sVhM4XA4fUQiEetLAgCuAeaBqa6uViKRSB/xeNz6kgCAa4D5S2ShUEihUMj6MgCAaww/BwMAMOH6GUxHR4dOnjyZvn3q1Cm1tLRo2LBhGjNmTL+OAwD4l+vAHD58WA8++GD6dlVVlSSpsrJSW7du7bdhAAB/cx2YBx54QI7jWGwBAGQR3oMBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJlx/Hgz85/MnyryekLEPFtV4PSEjM//1Ga8nZOTWKf/t9YSMdL173OsJuAyewQAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAw4SowsVhMM2bMUF5engoKCvTYY4/p+HE+CxsAcClXgWloaFA0GlVjY6P27Nmjixcvau7cuers7LTaBwDwqaCbk3fv3t3t9tatW1VQUKDm5mZ9/etf79dhAAB/cxWYv5dIJCRJw4YNu+I5yWRSyWQyfbutra0vlwQA+ETGb/KnUimtXr1a5eXlmjp16hXPi8ViCofD6SMSiWR6SQCAj2QcmGg0qmPHjmn79u09nlddXa1EIpE+4vF4ppcEAPhIRi+RrVixQm+88YYOHDig0aNH93huKBRSKBTKaBwAwL9cBcZxHH33u99VXV2d9u/fr/Hjx1vtAgD4nKvARKNRbdu2Ta+99pry8vJ09uxZSVI4HNaQIUNMBgIA/MnVezA1NTVKJBJ64IEHVFhYmD527NhhtQ8A4FOuXyIDAKA3+F1kAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYcPWBY/CnV/7t372ekLGlp//R6wkZuXXXB15PyMib//kbrydk5L7ov3g9IWM31r3j9QQzPIMBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATrgJTU1Oj4uJi5efnKz8/X2VlZdq1a5fVNgCAj7kKzOjRo7Vu3To1Nzfr8OHDeuihh/Too4/q3XfftdoHAPCpoJuTFyxY0O32D37wA9XU1KixsVFTpkzp12EAAH9zFZi/1dXVpV/84hfq7OxUWVnZFc9LJpNKJpPp221tbZleEgDgI67f5D969KhuvvlmhUIhPfXUU6qrq9PkyZOveH4sFlM4HE4fkUikT4MBAP7gOjB33nmnWlpa9M4772j58uWqrKzUe++9d8Xzq6urlUgk0kc8Hu/TYACAP7h+iSw3N1cTJkyQJE2fPl1NTU16/vnntXHjxsueHwqFFAqF+rYSAOA7ff45mFQq1e09FgAAJJfPYKqrq1VRUaExY8aovb1d27Zt0/79+1VfX2+1DwDgU64C09raqm9961v69NNPFQ6HVVxcrPr6ej388MNW+wAAPuUqMFu2bLHaAQDIMvwuMgCACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATLj6wLHr3f98Y5bXEzJyxw0tXk/I2LlvF3k9ISP/FcvzesJ15czXA15PyNiEOq8X2OEZDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmOhTYNatW6dAIKDVq1f30xwAQLbIODBNTU3auHGjiouL+3MPACBLZBSYjo4OLVmyRJs3b9bQoUP7exMAIAtkFJhoNKr58+drzpw5/b0HAJAlgm4fsH37dh05ckRNTU29Oj+ZTCqZTKZvt7W1ub0kAMCHXD2DicfjWrVqlX72s59p8ODBvXpMLBZTOBxOH5FIJKOhAAB/cRWY5uZmtba26p577lEwGFQwGFRDQ4NeeOEFBYNBdXV1XfKY6upqJRKJ9BGPx/ttPADg2uXqJbLZs2fr6NGj3e5bunSpJk2apGeffVY5OTmXPCYUCikUCvVtJQDAd1wFJi8vT1OnTu1230033aThw4dfcj8A4PrGT/IDAEy4/i6yv7d///5+mAEAyDY8gwEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwESfP3DsenJj3TteT8jIlMeXeD0hYz/Y+ZrXEzLy2E0dXk+4rhQdcLyegMvgGQwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAE64C89xzzykQCHQ7Jk2aZLUNAOBjQbcPmDJlivbu3fvXLxB0/SUAANcB13UIBoMaNWqUxRYAQBZx/R7MiRMnVFRUpNtuu01LlizR6dOnezw/mUyqra2t2wEAyH6uAjNr1ixt3bpVu3fvVk1NjU6dOqX77rtP7e3tV3xMLBZTOBxOH5FIpM+jAQDXPleBqaio0MKFC1VcXKx58+bpzTff1Pnz5/Xqq69e8THV1dVKJBLpIx6P93k0AODa16d36G+55RbdcccdOnny5BXPCYVCCoVCfbkMAMCH+vRzMB0dHfrggw9UWFjYX3sAAFnCVWCeeeYZNTQ06A9/+IPefvttfeMb31BOTo4WL15stQ8A4FOuXiL7+OOPtXjxYv3pT3/SiBEjdO+996qxsVEjRoyw2gcA8ClXgdm+fbvVDgBAluF3kQEATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATrj4PBv40+p/f9XpCxmo0wesJGXnv93/2ekJGtux70OsJGZlQ1+j1BFwGz2AAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmHAdmE8++USPP/64hg8friFDhujuu+/W4cOHLbYBAHws6Obkzz//XOXl5XrwwQe1a9cujRgxQidOnNDQoUOt9gEAfMpVYH74wx8qEomotrY2fd/48eP7fRQAwP9cvUT2+uuvq7S0VAsXLlRBQYGmTZumzZs39/iYZDKptra2bgcAIPu5CsyHH36ompoaTZw4UfX19Vq+fLlWrlypl19++YqPicViCofD6SMSifR5NADg2hdwHMfp7cm5ubkqLS3V22+/nb5v5cqVampq0sGDBy/7mGQyqWQymb7d1tamSCSiB/SogoEb+jAduHbd//s/ez0hI1v2Pej1hIxMeLrR6wnXjS+ci9qv15RIJJSfn9/jua6ewRQWFmry5Mnd7rvrrrt0+vTpKz4mFAopPz+/2wEAyH6uAlNeXq7jx493u+/999/X2LFj+3UUAMD/XAXm6aefVmNjo9auXauTJ09q27Zt2rRpk6LRqNU+AIBPuQrMjBkzVFdXp5///OeaOnWqvv/972v9+vVasmSJ1T4AgE+5+jkYSXrkkUf0yCOPWGwBAGQRfhcZAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmXH/gGIDsdfNp/s2J/sPfJgCACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMOEqMOPGjVMgELjkiEajVvsAAD4VdHNyU1OTurq60rePHTumhx9+WAsXLuz3YQAAf3MVmBEjRnS7vW7dOt1+++26//77+3UUAMD/XAXmb124cEGvvPKKqqqqFAgErnheMplUMplM325ra8v0kgAAH8n4Tf6dO3fq/PnzeuKJJ3o8LxaLKRwOp49IJJLpJQEAPpJxYLZs2aKKigoVFRX1eF51dbUSiUT6iMfjmV4SAOAjGb1E9tFHH2nv3r361a9+ddVzQ6GQQqFQJpcBAPhYRs9gamtrVVBQoPnz5/f3HgBAlnAdmFQqpdraWlVWVioYzPh7BAAAWc51YPbu3avTp09r2bJlFnsAAFnC9VOQuXPnynEciy0AgCzC7yIDAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJgb8Iym//CyZL3RR4mNlkKX+0nHR6wkZ6Ur+xesJGfnC8eeftx99of/7s+7N54IFnAH+9LCPP/5YkUhkIC8JAOhn8Xhco0eP7vGcAQ9MKpXSmTNnlJeXp0Ag0K9fu62tTZFIRPF4XPn5+f36tS2xe2Cxe+D5dTu7L+U4jtrb21VUVKRBg3p+l2XAXyIbNGjQVavXV/n5+b76y/Aldg8sdg88v25nd3fhcLhX5/EmPwDABIEBAJjIqsCEQiGtWbNGoVDI6ymusHtgsXvg+XU7u/tmwN/kBwBcH7LqGQwA4NpBYAAAJggMAMAEgQEAmMiawGzYsEHjxo3T4MGDNWvWLB06dMjrSVd14MABLViwQEVFRQoEAtq5c6fXk3olFotpxowZysvLU0FBgR577DEdP37c61lXVVNTo+Li4vQPn5WVlWnXrl1ez3Jt3bp1CgQCWr16tddTevTcc88pEAh0OyZNmuT1rF755JNP9Pjjj2v48OEaMmSI7r77bh0+fNjrWVc1bty4S/7MA4GAotGoJ3uyIjA7duxQVVWV1qxZoyNHjqikpETz5s1Ta2ur19N61NnZqZKSEm3YsMHrKa40NDQoGo2qsbFRe/bs0cWLFzV37lx1dnZ6Pa1Ho0eP1rp169Tc3KzDhw/roYce0qOPPqp3333X62m91tTUpI0bN6q4uNjrKb0yZcoUffrpp+njd7/7ndeTrurzzz9XeXm5brjhBu3atUvvvfeefvSjH2no0KFeT7uqpqambn/ee/bskSQtXLjQm0FOFpg5c6YTjUbTt7u6upyioiInFot5uModSU5dXZ3XMzLS2trqSHIaGhq8nuLa0KFDnZ/+9Kdez+iV9vZ2Z+LEic6ePXuc+++/31m1apXXk3q0Zs0ap6SkxOsZrj377LPOvffe6/WMfrFq1Srn9ttvd1KplCfX9/0zmAsXLqi5uVlz5sxJ3zdo0CDNmTNHBw8e9HDZ9SORSEiShg0b5vGS3uvq6tL27dvV2dmpsrIyr+f0SjQa1fz587v9Xb/WnThxQkVFRbrtttu0ZMkSnT592utJV/X666+rtLRUCxcuVEFBgaZNm6bNmzd7Pcu1Cxcu6JVXXtGyZcv6/RcL95bvA/PZZ5+pq6tLI0eO7Hb/yJEjdfbsWY9WXT9SqZRWr16t8vJyTZ061es5V3X06FHdfPPNCoVCeuqpp1RXV6fJkyd7Peuqtm/friNHjigWi3k9pddmzZqlrVu3avfu3aqpqdGpU6d03333qb293etpPfrwww9VU1OjiRMnqr6+XsuXL9fKlSv18ssvez3NlZ07d+r8+fN64oknPNsw4L9NGdklGo3q2LFjvnhtXZLuvPNOtbS0KJFI6Je//KUqKyvV0NBwTUcmHo9r1apV2rNnjwYPHuz1nF6rqKhI/+/i4mLNmjVLY8eO1auvvqpvf/vbHi7rWSqVUmlpqdauXStJmjZtmo4dO6YXX3xRlZWVHq/rvS1btqiiokJFRUWebfD9M5hbb71VOTk5OnfuXLf7z507p1GjRnm06vqwYsUKvfHGG3rrrbfMP4Khv+Tm5mrChAmaPn26YrGYSkpK9Pzzz3s9q0fNzc1qbW3VPffco2AwqGAwqIaGBr3wwgsKBoPq6uryemKv3HLLLbrjjjt08uRJr6f0qLCw8JJ/cNx1112+eHnvSx999JH27t2rJ5980tMdvg9Mbm6upk+frn379qXvS6VS2rdvn29eW/cbx3G0YsUK1dXV6be//a3Gjx/v9aSMpVIpJZNJr2f0aPbs2Tp69KhaWlrSR2lpqZYsWaKWlhbl5OR4PbFXOjo69MEHH6iwsNDrKT0qLy+/5Nvu33//fY0dO9ajRe7V1taqoKBA8+fP93RHVrxEVlVVpcrKSpWWlmrmzJlav369Ojs7tXTpUq+n9aijo6Pbv+ZOnTqllpYWDRs2TGPGjPFwWc+i0ai2bdum1157TXl5een3usLhsIYMGeLxuiurrq5WRUWFxowZo/b2dm3btk379+9XfX2919N6lJeXd8n7WzfddJOGDx9+Tb/v9cwzz2jBggUaO3aszpw5ozVr1ignJ0eLFy/2elqPnn76aX3ta1/T2rVr9c1vflOHDh3Spk2btGnTJq+n9UoqlVJtba0qKysVDHr8n3hPvnfNwE9+8hNnzJgxTm5urjNz5kynsbHR60lX9dZbbzmSLjkqKyu9ntajy22W5NTW1no9rUfLli1zxo4d6+Tm5jojRoxwZs+e7fzmN7/xelZG/PBtyosWLXIKCwud3Nxc5ytf+YqzaNEi5+TJk17P6pVf//rXztSpU51QKORMmjTJ2bRpk9eTeq2+vt6R5Bw/ftzrKQ6/rh8AYML378EAAK5NBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAICJ/wWOC5Gg1yYw0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.images[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437, 10), (360, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=dataset.data\n",
    "Y=dataset.target\n",
    "Y=np.eye(10)[Y]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X))\n",
    "\n",
    "def root_mean_square_error(Y_gt,Y_pred):\n",
    "    return np.sqrt(np.mean((Y_gt-Y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "η=0.001 #learning rate\n",
    "\n",
    "D_in=X_train.shape[1]\n",
    "H1=128\n",
    "H2=32\n",
    "D_out=Y_train.shape[1] # =len(np.unique(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=np.random.randn(D_in,H1)\n",
    "W2=np.random.randn(H1,H2)\n",
    "W3=np.random.randn(H2,D_out)\n",
    "# len(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1=np.random.randn(1,H1)\n",
    "B2=np.random.randn(1,H2)\n",
    "B3=np.random.randn(1,D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy_train             loss_train              accuracy_test              loss_test        \n",
      "   0.16701461377870563      0.32849754638860484      0.3055555555555556       0.2864149689534019    \n",
      "   0.3959638135003479       0.27405587537215154             0.425             0.2649963469191287    \n",
      "    0.510786360473208       0.25485984520956334      0.5388888888888889       0.2513167272202748    \n",
      "   0.5845511482254697       0.24190228341911585      0.5722222222222222       0.24299262604721591   \n",
      "   0.6353514265831594       0.2314186728606381       0.5944444444444444       0.23580356893194568   \n",
      "   0.6722338204592901       0.22228941811308953      0.6361111111111111       0.2289160323851651    \n",
      "   0.7007654836464857       0.21420759748351292      0.6555555555555556       0.22270903188521118   \n",
      "   0.7272094641614475       0.20654115003455747      0.6638888888888889       0.2171378738996359    \n",
      "   0.7501739735560194       0.19990812187690787      0.6805555555555556       0.21207756803281247   \n",
      "   0.7703549060542797       0.19390934187190342      0.6861111111111111       0.20762790106336118   \n",
      "   0.7905358385525401       0.18830015543565995      0.7138888888888889       0.20354656078052275   \n",
      "   0.8100208768267223       0.18311637350366786      0.7333333333333333       0.19979130031290568   \n",
      "   0.8211551844119693       0.17827213187767155      0.7472222222222222       0.1960991615844332    \n",
      "   0.8350730688935282       0.17377533369846437      0.7444444444444445       0.19269670705310554   \n",
      "   0.8427279053583855       0.1695243425079507       0.7555555555555555       0.18977848530193628   \n",
      "   0.8455114822546973       0.16561171057304416      0.7638888888888888       0.18709504500927088   \n",
      "   0.8503827418232429       0.1619404956496491       0.7666666666666667       0.1844004695349488    \n",
      "   0.8538622129436325       0.15847578487982797      0.7666666666666667       0.18183754465497343   \n",
      "   0.8615170494084899       0.15504176115582272      0.7777777777777778       0.17946394308676147   \n",
      "   0.8691718858733473       0.15166107558097902      0.7777777777777778       0.17727607693217112   \n",
      "   0.8768267223382046       0.14844754398434926      0.7861111111111111       0.17503530270494538   \n",
      "   0.8858733472512178       0.14538661823097007      0.7888888888888889       0.1726569043130161    \n",
      "   0.8900487125956854       0.14242894267255007      0.8027777777777778       0.17014937282962997   \n",
      "   0.8956158663883089       0.13934126672887603      0.8138888888888889       0.1680070021359035    \n",
      "   0.9032707028531664       0.13631510669358518             0.825             0.1660595909578103    \n",
      "    0.907446068197634       0.1334035578841438       0.8277777777777777       0.16424143528024598   \n",
      "   0.9123173277661796       0.13053735823298745      0.8333333333333334       0.16262984312887052   \n",
      "   0.9171885873347251        0.127806315429835       0.8388888888888889       0.16121030854093082   \n",
      "   0.9227557411273486       0.1251514831439614       0.8361111111111111       0.15994845823990017   \n",
      "   0.9276270006958942       0.1226083072793418       0.8444444444444444       0.15881828199594253   \n",
      "   0.9311064718162839       0.1201905968492292       0.8472222222222222       0.1577635730738228    \n",
      "   0.9338900487125957       0.1178580097876067       0.8444444444444444       0.15667487156056045   \n",
      "   0.9373695198329853       0.11557317647570273      0.8472222222222222       0.15551831267199437   \n",
      "   0.9436325678496869       0.11329987281435591      0.8472222222222222        0.154374725456054    \n",
      "   0.9478079331941545       0.11100994156331705      0.8472222222222222       0.15327504959293173   \n",
      "   0.9491997216423104       0.10871071113519944      0.8472222222222222       0.1522492287528818    \n",
      "    0.953375086986778       0.1064234327728426       0.8444444444444444       0.1512984690457967    \n",
      "   0.9561586638830898       0.10417674970395001             0.85              0.1503396972235228    \n",
      "   0.9589422407794015       0.10209907489260878      0.8527777777777777       0.14931687380556785   \n",
      "   0.9603340292275574       0.10019178402521053      0.8555555555555555       0.14833638845134234   \n",
      "   0.9610299234516354       0.09838177531647312      0.8583333333333333       0.14742375105084463   \n",
      "    0.964509394572025       0.09666722133641421      0.8583333333333333       0.14654950951152812   \n",
      "    0.965205288796103       0.09503289658635927      0.8583333333333333       0.14571319682908013   \n",
      "   0.9672929714683368       0.09342132376793251      0.8583333333333333       0.14492032403454275   \n",
      "   0.9686847599164927       0.09184623522431808      0.8611111111111112       0.14414925670955583   \n",
      "   0.9693806541405706       0.09039800589751053      0.8611111111111112       0.1434047004968615    \n",
      "   0.9707724425887265       0.08903015429512444      0.8611111111111112       0.14271243238280718   \n",
      "   0.9728601252609603       0.08772020804910056      0.8638888888888889       0.14209424675013563   \n",
      "   0.9721642310368824       0.08640222840927149      0.8666666666666667       0.14152889269994598   \n",
      "   0.9728601252609603       0.08510537446334714      0.8666666666666667       0.14092424782757626   \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'accuracy_train':^25}{'loss_train':^25}{'accuracy_test':^25}{'loss_test':^25}\")\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    Y_pred_train = []\n",
    "    for x,y in zip(X_train,Y_train):\n",
    "\n",
    "        x = x.reshape(-1,1)\n",
    "\n",
    "        # forward\n",
    "\n",
    "        # layer1\n",
    "        out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "        # layer2\n",
    "        out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "        # layer3\n",
    "        out3 = softmax(out2 @ W3 + B3)\n",
    "        y_pred = out3\n",
    "        Y_pred_train.append(y_pred)\n",
    "\n",
    "        # backward مشتق گیری و محاسبه گرادیان برای مرحله بعد\n",
    "\n",
    "        # layer3\n",
    "        # گرادیان = ارور * مشتق\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_B3 = error\n",
    "        grad_W3 = out2.T @ error\n",
    "\n",
    "        # layer2\n",
    "        error = error @ W3.T * out2 *(1 - out2)\n",
    "        grad_B2 = error\n",
    "        grad_W2 = out1.T @ error\n",
    "\n",
    "        # layer1\n",
    "\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_B1 = error\n",
    "        grad_W1 = x @ error\n",
    "\n",
    "        # update\n",
    "\n",
    "        # layer1\n",
    "        W1 -= η * grad_W1\n",
    "        B1 -= η * grad_B1\n",
    "\n",
    "        # layer2\n",
    "        W2 -= η * grad_W2\n",
    "        B2 -= η * grad_B2\n",
    "\n",
    "        # layer3\n",
    "        W3 -= η * grad_W3\n",
    "        B3 -= η * grad_B3\n",
    "\n",
    "    # test\n",
    "    Y_pred_test=[]\n",
    "    for x,y in zip(X_test,Y_test):\n",
    "\n",
    "        x = x.reshape(-1,1)\n",
    "\n",
    "        # forward\n",
    "\n",
    "        # layer1\n",
    "        out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "        # layer2\n",
    "        out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "        # layer3\n",
    "        out3 = softmax(out2 @ W3 + B3)\n",
    "        y_pred = out3\n",
    "        Y_pred_test.append(y_pred)\n",
    "        \n",
    "        \n",
    "    Y_pred_train = np.array(Y_pred_train).reshape(-1,10)\n",
    "    loss_train=root_mean_square_error(Y_train,Y_pred_train)\n",
    "    accuracy_train= np.sum(np.argmax(Y_train,axis=1)==np.argmax(Y_pred_train,axis=1))/len(Y_train)\n",
    "\n",
    "    Y_pred_test = np.array(Y_pred_test).reshape(-1,10)\n",
    "    loss_test=root_mean_square_error(Y_test,Y_pred_test)\n",
    "    accuracy_test= np.sum(np.argmax(Y_test,axis=1)==np.argmax(Y_pred_test,axis=1))/len(Y_test)\n",
    "    \n",
    "    print(f\"{accuracy_train:^25}{loss_train:^25}{accuracy_test:^25}{loss_test:^25}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.064393   0.21158792 0.12262393 0.00466936 0.00437679 0.01081823\n",
      "  0.05247875 0.51148641 0.01344398 0.00412162]]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"test3.png\")\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "image = image.reshape(64,1)\n",
    "\n",
    "x = image\n",
    "# forward\n",
    "\n",
    "# layer1\n",
    "out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "# layer2\n",
    "out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "# layer3\n",
    "out3 = softmax(out2 @ W3 + B3)\n",
    "y_pred = out3\n",
    "\n",
    "print(y_pred)\n",
    "print(np.argmax(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
