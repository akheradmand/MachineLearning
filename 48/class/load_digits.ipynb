{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_digits()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[100]\n",
    "dataset.images[100]\n",
    "dataset.target[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c5616d30d0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.images[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437, 10), (360, 10))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=dataset.data\n",
    "Y=dataset.target\n",
    "Y=np.eye(10)[Y]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X))\n",
    "\n",
    "def root_mean_square_error(Y_gt,Y_pred):\n",
    "    return np.sqrt(np.mean((Y_gt-Y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "η=0.001 #learning rate\n",
    "\n",
    "D_in=X_train.shape[1]\n",
    "H1=128\n",
    "H2=32\n",
    "D_out=Y_train.shape[1] # =len(np.unique(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=np.random.randn(D_in,H1)\n",
    "W2=np.random.randn(H1,H2)\n",
    "W3=np.random.randn(H2,D_out)\n",
    "# len(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1=np.random.randn(1,H1)\n",
    "B2=np.random.randn(1,H2)\n",
    "B3=np.random.randn(1,D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy_train             loss_train              accuracy_test              loss_test        \n",
      "   0.20389700765483645      0.3162987540439739       0.33611111111111114      0.28757489721930796   \n",
      "   0.38900487125956856      0.27684154496066427      0.3888888888888889       0.27375688797288106   \n",
      "   0.4906054279749478       0.2583296561474144       0.46111111111111114      0.25986323180938803   \n",
      "   0.5706332637439109       0.24289874282069826             0.525             0.24893522239919275   \n",
      "    0.627000695894224       0.2295386796454842       0.5638888888888889       0.23933839450190678   \n",
      "   0.6778009742519137       0.21811346894508785      0.6083333333333333       0.23078595109951527   \n",
      "   0.7153792623521225       0.20831710743066548      0.6444444444444445       0.22288536466882652   \n",
      "   0.7446068197633959       0.19910079404046468             0.675             0.21612577520877357   \n",
      "   0.7717466945024356       0.19063919576539243             0.675              0.210879635808086    \n",
      "   0.7988865692414753       0.18324878236434838      0.6833333333333333       0.2063053283005261    \n",
      "    0.813500347947112       0.17667401987929948      0.7194444444444444       0.20226202759291964   \n",
      "   0.8274182324286709       0.17075255617782012      0.7388888888888889       0.1988175575161452    \n",
      "    0.837160751565762       0.16537260320657254      0.7472222222222222       0.19570029416250107   \n",
      "   0.8531663187195546       0.16009405043673214             0.75              0.19261945548025944   \n",
      "   0.8691718858733473       0.15505705173626394      0.7611111111111111       0.18966193033646883   \n",
      "   0.8747390396659708       0.15039369418839035      0.7722222222222223       0.18725262429584816   \n",
      "   0.8865692414752957       0.1457329876083627       0.7722222222222223       0.1844966344436657    \n",
      "   0.8928322894919972       0.1413276121663619              0.775             0.18193722387766306   \n",
      "   0.8977035490605428       0.13721575988720258      0.7916666666666666       0.17972154055541162   \n",
      "    0.907446068197634       0.13354107257525993      0.7972222222222223       0.17757021916423701   \n",
      "   0.9130132219902575       0.13012611251673553              0.8              0.1754411022767751    \n",
      "   0.9178844815588031       0.1268973354880306       0.8083333333333333        0.173350767525639    \n",
      "   0.9206680584551148       0.12383464080864562      0.8055555555555556       0.1715059369160606    \n",
      "   0.9241475295755045       0.12099891662144252      0.8111111111111111       0.16954593452213415   \n",
      "   0.9283228949199722       0.1183604050739835       0.8111111111111111       0.16769542746286462   \n",
      "   0.9338900487125957       0.11588293794445233      0.8083333333333333       0.1660269704776762    \n",
      "   0.9366736256089074       0.11351259144551756      0.8111111111111111       0.16447941514081288   \n",
      "   0.9394572025052192       0.1112137227271639       0.8111111111111111       0.16303773446138933   \n",
      "    0.940848990953375       0.10901776372304915      0.8138888888888889       0.1617206798120923    \n",
      "   0.9457202505219207       0.10692325852544889      0.8166666666666667       0.1604862771698689    \n",
      "   0.9498956158663883       0.1049141652029426              0.825             0.15918352117578552   \n",
      "   0.9512874043145442       0.10297693382981699      0.8277777777777777       0.15769844383453976   \n",
      "    0.953375086986778       0.10112970810215108      0.8333333333333334       0.15621438041109945   \n",
      "   0.9554627696590118       0.0993041696154247       0.8361111111111111       0.1547706560086715    \n",
      "   0.9568545581071677       0.09752914186403312      0.8416666666666667       0.15338864301352956   \n",
      "   0.9568545581071677       0.09583841161797702      0.8472222222222222       0.1521016026291886    \n",
      "   0.9582463465553236       0.0942350244379227       0.8472222222222222       0.15090701649547114   \n",
      "   0.9603340292275574       0.09269736625156878      0.8472222222222222       0.14979809525059593   \n",
      "   0.9617258176757133       0.09120208063164177             0.85              0.14876066721569092   \n",
      "   0.9638135003479471       0.08976932749679575             0.85              0.14776341330454512   \n",
      "    0.964509394572025       0.08840719721388512             0.85              0.14679761330646598   \n",
      "   0.9665970772442589       0.08710429975600464             0.85              0.14587214151885383   \n",
      "   0.9672929714683368       0.08586830736999139             0.85              0.14499858843241814   \n",
      "   0.9686847599164927       0.08470723928964437             0.85              0.14418089431363137   \n",
      "   0.9700765483646486       0.08360900813930397             0.85              0.14342216110702355   \n",
      "   0.9714683368128044       0.08256076311383642      0.8472222222222222       0.14273473441293905   \n",
      "   0.9721642310368824        0.081554491645606              0.85              0.14214815192940589   \n",
      "   0.9728601252609603       0.08056600968074953      0.8527777777777777       0.14165381520468498   \n",
      "   0.9728601252609603       0.07953645798179702      0.8555555555555555       0.1411781929253495    \n",
      "   0.9735560194850382       0.07850536115904913      0.8555555555555555       0.1407047040916852    \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'accuracy_train':^25}{'loss_train':^25}{'accuracy_test':^25}{'loss_test':^25}\")\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    Y_pred_train = []\n",
    "    for x,y in zip(X_train,Y_train):\n",
    "\n",
    "        x = x.reshape(-1,1)\n",
    "\n",
    "        # forward\n",
    "\n",
    "        # layer1\n",
    "        out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "        # layer2\n",
    "        out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "        # layer3\n",
    "        out3 = softmax(out2 @ W3 + B3)\n",
    "        y_pred = out3\n",
    "        Y_pred_train.append(y_pred)\n",
    "\n",
    "        # backward مشتق گیری و محاسبه گرادیان برای مرحله بعد\n",
    "\n",
    "        # layer3\n",
    "        # گرادیان = ارور * مشتق\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_B3 = error\n",
    "        grad_W3 = out2.T @ error\n",
    "\n",
    "        # layer2\n",
    "        error = error @ W3.T * out2 *(1 - out2)\n",
    "        grad_B2 = error\n",
    "        grad_W2 = out1.T @ error\n",
    "\n",
    "        # layer1\n",
    "\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_B1 = error\n",
    "        grad_W1 = x @ error\n",
    "\n",
    "        # update\n",
    "\n",
    "        # layer1\n",
    "        W1 -= η * grad_W1\n",
    "        B1 -= η * grad_B1\n",
    "\n",
    "        # layer2\n",
    "        W2 -= η * grad_W2\n",
    "        B2 -= η * grad_B2\n",
    "\n",
    "        # layer3\n",
    "        W3 -= η * grad_W3\n",
    "        B3 -= η * grad_B3\n",
    "\n",
    "    # test\n",
    "    Y_pred_test=[]\n",
    "    for x,y in zip(X_test,Y_test):\n",
    "\n",
    "        x = x.reshape(-1,1)\n",
    "\n",
    "        # forward\n",
    "\n",
    "        # layer1\n",
    "        out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "        # layer2\n",
    "        out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "        # layer3\n",
    "        out3 = softmax(out2 @ W3 + B3)\n",
    "        y_pred = out3\n",
    "        Y_pred_test.append(y_pred)\n",
    "        \n",
    "        \n",
    "    Y_pred_train = np.array(Y_pred_train).reshape(-1,10)\n",
    "    loss_train=root_mean_square_error(Y_train,Y_pred_train)\n",
    "    accuracy_train= np.sum(np.argmax(Y_train,axis=1)==np.argmax(Y_pred_train,axis=1))/len(Y_train)\n",
    "\n",
    "    Y_pred_test = np.array(Y_pred_test).reshape(-1,10)\n",
    "    loss_test=root_mean_square_error(Y_test,Y_pred_test)\n",
    "    accuracy_test= np.sum(np.argmax(Y_test,axis=1)==np.argmax(Y_pred_test,axis=1))/len(Y_test)\n",
    "    \n",
    "    print(f\"{accuracy_train:^25}{loss_train:^25}{accuracy_test:^25}{loss_test:^25}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.59494303e-03 1.85864490e-02 2.57232612e-02 3.03201194e-03\n",
      "  2.52940274e-01 1.40941503e-05 3.03774277e-03 6.71069425e-01\n",
      "  4.64393717e-04 2.25374053e-02]]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"test3.png\")\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "image = image.reshape(64,1)\n",
    "\n",
    "x = image\n",
    "# forward\n",
    "\n",
    "# layer1\n",
    "out1 = sigmoid(x.T @ W1 + B1)\n",
    "\n",
    "# layer2\n",
    "out2 = sigmoid(out1 @ W2 + B2)\n",
    "\n",
    "# layer3\n",
    "out3 = softmax(out2 @ W3 + B3)\n",
    "y_pred = out3\n",
    "\n",
    "print(y_pred)\n",
    "print(np.argmax(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
